{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import sys \n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.svm import SVR\n",
    "from video_process_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'GDI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_processed =\\\n",
    "    pd.read_csv(\"./data/processed/alldata_processed_with_dev_residual.csv\" )\n",
    "alldata_processed['videoid'] = alldata_processed['videoid'].apply(lambda x: int(x))\n",
    "alldata_processed['target_count'] = alldata_processed.groupby('videoid')[target_col].transform(lambda x: x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasplit_df = pd.read_csv('%sdata/processed/train_test_valid_id_split.csv' % (HOME_DIR))\n",
    "datasplit_df['videoid'] = datasplit_df['videoid'].apply(lambda x: int(x))\n",
    "all_ids = set(datasplit_df['videoid'])\n",
    "train_ids = set(datasplit_df[datasplit_df['dataset'] == 'train']['videoid'])\n",
    "validation_ids = set(datasplit_df[datasplit_df['dataset'] == 'validation']['videoid'])\n",
    "test_ids = set(datasplit_df[datasplit_df['dataset'] == 'test']['videoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/all_processed_videos.pickle', 'rb') as handle:\n",
    "    processed_videos = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_video_ids = [x[0] for x in processed_videos if x[0] in all_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [x[1][:500,:] for x in processed_videos if x[0] in all_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGLE_ANK_KNE_HIP = 50\n",
    "RANGLE_ANK_KNE_HIP = 51\n",
    "LANGLE_BTO_ANK_KNE = 52\n",
    "RANGLE_BTO_ANK_KNE = 53\n",
    "LDIST_BTO_ANK = 54\n",
    "RDIST_BTO_ANK = 55\n",
    "XDIST_LANK_RANK = 56\n",
    "XDIST_RANK_LANK = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(processed_video_ids,columns=['videoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentiles_xy(df,videos,column_left,column_right,column_name,percentile):\n",
    "    df = df.copy()\n",
    "    name_base_L = 'p%s_L%s' % (percentile,column_name)\n",
    "    name_base_R = 'p%s_R%s' % (percentile,column_name)\n",
    "    df[name_base_L + '_x'] = [np.percentile(v[:,2*column_left],percentile) for v in videos]\n",
    "    df[name_base_R + '_x'] = [np.percentile(v[:,2*column_right],percentile) for v in videos]\n",
    "    df[name_base_L + '_y'] = [np.percentile(v[:,2*column_left+1],percentile) for v in videos]\n",
    "    df[name_base_R + '_y'] = [np.percentile(v[:,2*column_right+1],percentile) for v in videos]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentiles(df,videos,column_idx,column_name,percentile):\n",
    "    df[column_name] = [np.percentile(v[:,column_idx],percentile) for v in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(df,videos,col_name,col_idx,fn):\n",
    "    df[col_name] = [fn(v[:,col_idx]) for v in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percentile in [10,25,50,75,90]:\n",
    "    fn = lambda x: np.percentile(x,percentile)\n",
    "    for keypoint,idx in [('LANK',LANK),('RANK',RANK),('LKNE',LKNE),('RKNE',RKNE),\n",
    "                         ('LHIP',LHIP),('RHIP',RHIP),('LBTO',LBTO),('RBTO',RBTO)]:\n",
    "        apply_transform(features_df,videos,'p%s_%s_x' % (percentile,keypoint),2*idx,fn)\n",
    "        apply_transform(features_df,videos,'p%s_%s_y' % (percentile,keypoint),2*idx+1,fn)\n",
    "        \n",
    "    for keypoint,idx in [('LANGLE_ANK_KNE_HIP',LANGLE_ANK_KNE_HIP),('RANGLE_ANK_KNE_HIP',RANGLE_ANK_KNE_HIP),\n",
    "                         ('LANGLE_BTO_ANK_KNE',LANGLE_BTO_ANK_KNE),('RANGLE_BTO_ANK_KNE',RANGLE_BTO_ANK_KNE),\n",
    "                         ('LDIST_BTO_ANK',LDIST_BTO_ANK),('RDIST_BTO_ANK',RDIST_BTO_ANK),\n",
    "                         ('XDIST_LANK_RANK',XDIST_LANK_RANK),('XDIST_RANK_LANK',XDIST_RANK_LANK)]:\n",
    "        apply_transform(features_df,videos,'p%s_%s' % (percentile,keypoint),idx,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = np.std\n",
    "for keypoint,idx in [('LANK',LANK),('RANK',RANK),('LKNE',LKNE),('RKNE',RKNE),\n",
    "                     ('LHIP',LHIP),('RHIP',RHIP),('LBTO',LBTO),('RBTO',RBTO)]:\n",
    "    apply_transform(features_df,videos,'std_%s_x' % (keypoint),2*idx,fn)\n",
    "    apply_transform(features_df,videos,'std_%s_y' % (keypoint),2*idx+1,fn)\n",
    "\n",
    "for keypoint,idx in [('LANGLE_ANK_KNE_HIP',LANGLE_ANK_KNE_HIP),('RANGLE_ANK_KNE_HIP',RANGLE_ANK_KNE_HIP),\n",
    "                     ('LANGLE_BTO_ANK_KNE',LANGLE_BTO_ANK_KNE),('RANGLE_BTO_ANK_KNE',RANGLE_BTO_ANK_KNE),\n",
    "                     ('LDIST_BTO_ANK',LDIST_BTO_ANK),('RDIST_BTO_ANK',RDIST_BTO_ANK),\n",
    "                     ('XDIST_LANK_RANK',XDIST_LANK_RANK),('XDIST_RANK_LANK',XDIST_RANK_LANK)]:\n",
    "    apply_transform(features_df,videos,'std_%s' % (keypoint),idx,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orient_columns(df,left_col_name,right_col_name,col_name):\n",
    "    df[col_name] = df.apply(lambda row: row[left_col_name] if row.side == 'L' else\n",
    "                                           row[right_col_name],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = features_df.merge(right=alldata_processed[['side','videoid',target_col,\"cadence\",\"speed\",\"height\"]],on=['videoid'],how='inner')\n",
    "final_df = final_df.merge(right=datasplit_df[['videoid','dataset']],on=['videoid'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = []\n",
    "for percentile in [10,25,50,75,90]:\n",
    "    for keypoint in ['ANK','HIP','KNE','BTO']:\n",
    "        orient_columns(final_df,'p%s_L%s_x' % (percentile,keypoint),\n",
    "                       'p%s_R%s_x' % (percentile,keypoint),\n",
    "                       'p%s_%s_x' % (percentile,keypoint))\n",
    "        orient_columns(final_df,'p%s_L%s_y' % (percentile,keypoint),\n",
    "                       'p%s_R%s_y' % (percentile,keypoint),\n",
    "                       'p%s_%s_y' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s_x' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s_y' % (percentile,keypoint))\n",
    "        \n",
    "    for keypoint in ['ANGLE_ANK_KNE_HIP','ANGLE_BTO_ANK_KNE','DIST_BTO_ANK']:\n",
    "        orient_columns(final_df,'p%s_L%s' % (percentile,keypoint),\n",
    "                       'p%s_R%s' % (percentile,keypoint),\n",
    "                       'p%s_%s' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s' % (percentile,keypoint))  \n",
    "        \n",
    "    orient_columns(final_df,'p%s_XDIST_LANK_RANK' % (percentile),\n",
    "                            'p%s_XDIST_RANK_LANK' % (percentile),\n",
    "                            'p%s_XDIST_LANK_RANK' %(percentile))\n",
    "    Xcols.append('p%s_XDIST_LANK_RANK' %(percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keypoint in ['ANK','HIP','KNE','BTO']:\n",
    "    orient_columns(final_df,'std_L%s_x' % (keypoint),\n",
    "                   'std_R%s_x' % (keypoint),\n",
    "                   'std_%s_x' % (keypoint))\n",
    "    orient_columns(final_df,'std_L%s_y' % (keypoint),\n",
    "                   'std_R%s_y' % (keypoint),\n",
    "                   'std_%s_y' % (keypoint))\n",
    "    Xcols.append('std_%s_x' % (keypoint))\n",
    "    Xcols.append('std_%s_y' % (keypoint))\n",
    "\n",
    "for keypoint in ['ANGLE_ANK_KNE_HIP','ANGLE_BTO_ANK_KNE','DIST_BTO_ANK']:\n",
    "    orient_columns(final_df,'std_L%s' % (keypoint),\n",
    "                   'std_R%s' % (keypoint),\n",
    "                   'std_%s' % (keypoint))\n",
    "    Xcols.append('std_%s' % (keypoint))  \n",
    "\n",
    "orient_columns(final_df,'std_XDIST_LANK_RANK' ,\n",
    "                        'std_XDIST_RANK_LANK' ,\n",
    "                        'std_XDIST_LANK_RANK')\n",
    "Xcols.append('std_XDIST_LANK_RANK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = final_df[final_df['dataset'] == 'train'][Xcols].values\n",
    "y_train = final_df[final_df['dataset'] == 'train'][target_col].values\n",
    "\n",
    "X = final_df[Xcols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "rr = Ridge()\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rr = Pipeline([('sc', sc), ('rr', rr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(mod,df):\n",
    "    df['%s_pred' % (target_col)] = mod.predict(X)\n",
    "    metrics = {}\n",
    "    for dataset in ['train','validation','test']:\n",
    "        tmp = df[df['dataset'] == dataset]\n",
    "        c = tmp.corr()['%s' % (target_col)]['%s_pred' % (target_col)]\n",
    "        rmse =  np.sqrt(mean_squared_error(tmp['%s_pred' % (target_col)],\n",
    "                                           tmp['%s' % (target_col)]))\n",
    "        metrics[dataset] = (c,rmse)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rr = []\n",
    "for alpha in [0.001,0.01,0.1,1.0,10,100,1000,10000]:\n",
    "    print(alpha)\n",
    "    pipe_rr.set_params(rr__alpha=alpha).fit(X_train,y_train)\n",
    "    metrics = evaluate_model(pipe_rr,final_df)\n",
    "    results_rr.append((alpha,metrics['validation'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = results_rr[np.argmin([x[1] for x in results_rr])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rr.set_params(rr__alpha=best_alpha).fit(X_train,y_train)\n",
    "metrics = evaluate_model(pipe_rr,final_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rr feature importances\n",
    "feature_importances = pd.DataFrame(zip(pipe_rr.named_steps['rr'].coef_,Xcols),columns=['coef','feature'])\n",
    "feature_importances['abs_coef'] = np.abs(feature_importances['coef'])\n",
    "feature_importances.sort_values(by='abs_coef',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = list(range(10,110,10))\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "def random_search_rf_estimator(param_grid):\n",
    "    rf = RandomForestRegressor()\n",
    "    selected_params = {}\n",
    "    for k in param_grid.keys():\n",
    "        selected_params[k] = np.random.choice(param_grid[k])\n",
    "    rf.set_params(**selected_params)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = []\n",
    "np.random.seed(1)\n",
    "n_iters = 5\n",
    "for i in range(n_iters):\n",
    "    print(i)\n",
    "    rf = random_search_rf_estimator(param_grid)\n",
    "    rf.fit(X_train,y_train)   \n",
    "    metrics = evaluate_model(rf,final_df)\n",
    "    rf_results.append((rf.get_params(),metrics['validation'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_rf_params = rf_results[np.argmin([x[1] for x in rf_results])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.set_params(**optimal_rf_params)\n",
    "metrics = evaluate_model(rf,final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(Xcols,rf.feature_importances_),columns=['feature','feature_importance'])\n",
    "feature_importances.sort_values(by='feature_importance',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
