{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import sys \n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import cPickle as pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.svm import SVR\n",
    "from video_process_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_col = 'KneeFlex_maxExtension'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_processed =\\\n",
    "    pd.read_csv(\"data/annotations/alldata_processed_with_dev_residual.csv\" )\n",
    "alldata_processed['videoid'] = alldata_processed['videoid'].apply(lambda x: int(x))\n",
    "alldata_processed['target_count'] = alldata_processed.groupby('videoid')[target_col].transform(lambda x: x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home/bryan/clinical-video/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasplit_df = pd.read_csv('%sdata/train_test_valid_id_split.csv' % (HOME_DIR))\n",
    "datasplit_df['videoid'] = datasplit_df['videoid'].apply(lambda x: int(x))\n",
    "all_ids = set(datasplit_df['videoid'])\n",
    "train_ids = set(datasplit_df[datasplit_df['dataset'] == 'train']['videoid'])\n",
    "validation_ids = set(datasplit_df[datasplit_df['dataset'] == 'validation']['videoid'])\n",
    "test_ids = set(datasplit_df[datasplit_df['dataset'] == 'test']['videoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/all_processed_videos.pickle', 'r') as handle:\n",
    "    processed_videos = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_video_ids = [x[0] for x in processed_videos if x[0] in all_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "videos = [x[1][:500,:] for x in processed_videos if x[0] in all_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGLE_ANK_KNE_HIP = 50\n",
    "RANGLE_ANK_KNE_HIP = 51\n",
    "LANGLE_BTO_ANK_KNE = 52\n",
    "RANGLE_BTO_ANK_KNE = 53\n",
    "LDIST_BTO_ANK = 54\n",
    "RDIST_BTO_ANK = 55\n",
    "XDIST_LANK_RANK = 56\n",
    "XDIST_RANK_LANK = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(processed_video_ids,columns=['videoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_percentiles_xy(df,videos,column_left,column_right,column_name,percentile):\n",
    "    df = df.copy()\n",
    "    name_base_L = 'p%s_L%s' % (percentile,column_name)\n",
    "    name_base_R = 'p%s_R%s' % (percentile,column_name)\n",
    "    df[name_base_L + '_x'] = [np.percentile(v[:,2*column_left],percentile) for v in videos]\n",
    "    df[name_base_R + '_x'] = [np.percentile(v[:,2*column_right],percentile) for v in videos]\n",
    "    df[name_base_L + '_y'] = [np.percentile(v[:,2*column_left+1],percentile) for v in videos]\n",
    "    df[name_base_R + '_y'] = [np.percentile(v[:,2*column_right+1],percentile) for v in videos]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_percentiles(df,videos,column_idx,column_name,percentile):\n",
    "    df[column_name] = [np.percentile(v[:,column_idx],percentile) for v in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(df,videos,col_name,col_idx,fn):\n",
    "    df[col_name] = [fn(v[:,col_idx]) for v in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for percentile in [10,25,50,75,90]:\n",
    "    fn = lambda x: np.percentile(x,percentile)\n",
    "    for keypoint,idx in [('LANK',LANK),('RANK',RANK),('LKNE',LKNE),('RKNE',RKNE),\n",
    "                         ('LHIP',LHIP),('RHIP',RHIP),('LBTO',LBTO),('RBTO',RBTO)]:\n",
    "        apply_transform(features_df,videos,'p%s_%s_x' % (percentile,keypoint),2*idx,fn)\n",
    "        apply_transform(features_df,videos,'p%s_%s_y' % (percentile,keypoint),2*idx+1,fn)\n",
    "        \n",
    "    for keypoint,idx in [('LANGLE_ANK_KNE_HIP',LANGLE_ANK_KNE_HIP),('RANGLE_ANK_KNE_HIP',RANGLE_ANK_KNE_HIP),\n",
    "                         ('LANGLE_BTO_ANK_KNE',LANGLE_BTO_ANK_KNE),('RANGLE_BTO_ANK_KNE',RANGLE_BTO_ANK_KNE),\n",
    "                         ('LDIST_BTO_ANK',LDIST_BTO_ANK),('RDIST_BTO_ANK',RDIST_BTO_ANK),\n",
    "                         ('XDIST_LANK_RANK',XDIST_LANK_RANK),('XDIST_RANK_LANK',XDIST_RANK_LANK)]:\n",
    "        apply_transform(features_df,videos,'p%s_%s' % (percentile,keypoint),idx,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = np.std\n",
    "for keypoint,idx in [('LANK',LANK),('RANK',RANK),('LKNE',LKNE),('RKNE',RKNE),\n",
    "                     ('LHIP',LHIP),('RHIP',RHIP),('LBTO',LBTO),('RBTO',RBTO)]:\n",
    "    apply_transform(features_df,videos,'std_%s_x' % (keypoint),2*idx,fn)\n",
    "    apply_transform(features_df,videos,'std_%s_y' % (keypoint),2*idx+1,fn)\n",
    "\n",
    "for keypoint,idx in [('LANGLE_ANK_KNE_HIP',LANGLE_ANK_KNE_HIP),('RANGLE_ANK_KNE_HIP',RANGLE_ANK_KNE_HIP),\n",
    "                     ('LANGLE_BTO_ANK_KNE',LANGLE_BTO_ANK_KNE),('RANGLE_BTO_ANK_KNE',RANGLE_BTO_ANK_KNE),\n",
    "                     ('LDIST_BTO_ANK',LDIST_BTO_ANK),('RDIST_BTO_ANK',RDIST_BTO_ANK),\n",
    "                     ('XDIST_LANK_RANK',XDIST_LANK_RANK),('XDIST_RANK_LANK',XDIST_RANK_LANK)]:\n",
    "    apply_transform(features_df,videos,'std_%s' % (keypoint),idx,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orient_columns(df,left_col_name,right_col_name,col_name):\n",
    "    df[col_name] = df.apply(lambda row: row[left_col_name] if row.side == 'L' else\n",
    "                                           row[right_col_name],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = features_df.merge(right=alldata_processed[['side','videoid',target_col]],on=['videoid'],how='inner')\n",
    "final_df = final_df.merge(right=datasplit_df[['videoid','dataset']],on=['videoid'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = []\n",
    "for percentile in [10,25,50,75,90]:\n",
    "    for keypoint in ['ANK','HIP','KNE','BTO']:\n",
    "        orient_columns(final_df,'p%s_L%s_x' % (percentile,keypoint),\n",
    "                       'p%s_R%s_x' % (percentile,keypoint),\n",
    "                       'p%s_%s_x' % (percentile,keypoint))\n",
    "        orient_columns(final_df,'p%s_L%s_y' % (percentile,keypoint),\n",
    "                       'p%s_R%s_y' % (percentile,keypoint),\n",
    "                       'p%s_%s_y' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s_x' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s_y' % (percentile,keypoint))\n",
    "        \n",
    "    for keypoint in ['ANGLE_ANK_KNE_HIP','ANGLE_BTO_ANK_KNE','DIST_BTO_ANK']:\n",
    "        orient_columns(final_df,'p%s_L%s' % (percentile,keypoint),\n",
    "                       'p%s_R%s' % (percentile,keypoint),\n",
    "                       'p%s_%s' % (percentile,keypoint))\n",
    "        Xcols.append('p%s_%s' % (percentile,keypoint))  \n",
    "        \n",
    "    orient_columns(final_df,'p%s_XDIST_LANK_RANK' % (percentile),\n",
    "                            'p%s_XDIST_RANK_LANK' % (percentile),\n",
    "                            'p%s_XDIST_LANK_RANK' %(percentile))\n",
    "    Xcols.append('p%s_XDIST_LANK_RANK' %(percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for keypoint in ['ANK','HIP','KNE','BTO']:\n",
    "    orient_columns(final_df,'std_L%s_x' % (keypoint),\n",
    "                   'std_R%s_x' % (keypoint),\n",
    "                   'std_%s_x' % (keypoint))\n",
    "    orient_columns(final_df,'std_L%s_y' % (keypoint),\n",
    "                   'std_R%s_y' % (keypoint),\n",
    "                   'std_%s_y' % (keypoint))\n",
    "    Xcols.append('std_%s_x' % (keypoint))\n",
    "    Xcols.append('std_%s_y' % (keypoint))\n",
    "\n",
    "for keypoint in ['ANGLE_ANK_KNE_HIP','ANGLE_BTO_ANK_KNE','DIST_BTO_ANK']:\n",
    "    orient_columns(final_df,'std_L%s' % (keypoint),\n",
    "                   'std_R%s' % (keypoint),\n",
    "                   'std_%s' % (keypoint))\n",
    "    Xcols.append('std_%s' % (keypoint))  \n",
    "\n",
    "orient_columns(final_df,'std_XDIST_LANK_RANK' ,\n",
    "                        'std_XDIST_RANK_LANK' ,\n",
    "                        'std_XDIST_LANK_RANK')\n",
    "Xcols.append('std_XDIST_LANK_RANK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = final_df[final_df['dataset'] == 'train'][Xcols].values\n",
    "y_train = final_df[final_df['dataset'] == 'train'][target_col].values\n",
    "\n",
    "X = final_df[Xcols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "rr = Ridge()\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_rr = Pipeline([('sc', sc), ('rr', rr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(df):\n",
    "    metrics = {}\n",
    "    for dataset in ['train','validation','test']:\n",
    "        tmp = df[df['dataset'] == dataset]\n",
    "        c = tmp.corr()['%s' % (target_col)]['%s_pred' % (target_col)]\n",
    "        rmse =  np.sqrt(mean_squared_error(tmp['%s_pred' % (target_col)],\n",
    "                                           tmp['%s' % (target_col)]))\n",
    "        metrics[dataset] = (c,rmse)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1\n",
      "10\n",
      "100\n",
      "1000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "results_rr = []\n",
    "for alpha in [10**-4,10**-3,10**-2,10**-1,1,10*1,10**2,10**3,10**4]:\n",
    "    print(alpha)\n",
    "    pipe_rr.set_params(rr__alpha=alpha).fit(X_train,y_train)\n",
    "    final_df['%s_pred' % (target_col)] = pipe_rr.predict(X)\n",
    "    metrics = evaluate_model(final_df)\n",
    "    results_rr.append((alpha,metrics['validation'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_alpha = results_rr[np.argmin([x[1] for x in results_rr])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rr.set_params(rr__alpha=best_alpha).fit(X_train,y_train)\n",
    "final_df['%s_pred' % (target_col)] = pipe_rr.predict(X)\n",
    "final_df[['videoid','side','dataset','%s_pred' % (target_col)]].to_csv(\"predictions/rr_%s_predictions.csv\" % (target_col),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': (0.75502315399923603, 8.7375656943527016),\n",
       " 'train': (0.83160388542721797, 7.5479378748185759),\n",
       " 'validation': (0.85125236535821758, 6.8064237551978612)}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate_model(final_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rr feature importances\n",
    "feature_importances = pd.DataFrame(zip(pipe_rr.named_steps['rr'].coef_,Xcols),columns=['coef','feature'])\n",
    "feature_importances['abs_coef'] = np.abs(feature_importances['coef'])\n",
    "feature_importances.sort_values(by='abs_coef',ascending=False)\n",
    "feature_importances.to_csv(\"figures/feature_importances_%s_rr.csv\" % (target_col),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [100]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = range(10,110,10)\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "def random_search_rf_estimator(param_grid):\n",
    "    rf = RandomForestRegressor()\n",
    "    selected_params = {}\n",
    "    for k in param_grid.keys():\n",
    "        selected_params[k] = np.random.choice(param_grid[k])\n",
    "    rf.set_params(**selected_params)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "rf_results = []\n",
    "np.random.seed(1)\n",
    "n_iters = 20\n",
    "for i in range(n_iters):\n",
    "    print(i)\n",
    "    rf = random_search_rf_estimator(param_grid)\n",
    "    rf.fit(X_train,y_train)   \n",
    "    final_df['%s_pred' % (target_col)] = rf.predict(X)\n",
    "    metrics = evaluate_model(final_df)\n",
    "    rf_results.append((rf.get_params(),metrics['validation'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_rf_params = rf_results[np.argmin([x[1] for x in rf_results])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_samples_split': 5,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.set_params(**optimal_rf_params)\n",
    "final_df['%s_pred' % (target_col)] = rf.predict(X)\n",
    "final_df[['videoid','side','dataset','%s_pred' % (target_col)]].to_csv(\"predictions/rf_%s_predictions.csv\" % (target_col),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': (0.75296709123594585, 8.7854231616237826),\n",
       " 'train': (0.95221446767072915, 4.3570128250793712),\n",
       " 'validation': (0.83491874319200288, 7.1243564415992493)}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate_model(final_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(Xcols,rf.feature_importances_),columns=['feature','feature_importance'])\n",
    "feature_importances.sort_values(by='feature_importance',ascending=False)\n",
    "feature_importances.to_csv(\"figures/feature_importances_%s_rf.csv\" % (target_col),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
